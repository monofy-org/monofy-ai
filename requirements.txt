accelerate
xformers
edge-tts
openai-whisper
huggingface_hub
transformers
audiocraft
DeepCache
tomesd
omegaconf
gradio
fastapi
uvicorn
numpy
scipy
timm
pygame
emoji
rich
nudenet
optimum
imageio[ffmpeg]
insightface
ip_adapter
trimesh
pygltflib
mediapipe
civitdl
torchsde
moviepy
pdfrw
pytubefix
bs4

#TTS requirements
mutagen
coqpit
trainer
pypinyin
hangul_romanize
faster_whisper

# metavoice requirements
tyro
deepfilternet

#LGM requirements
kiui
rembg
roma
plyfile

#TSR requirements
git+https://github.com/tatsy/torchmcubes.git

# github repos
diffusers @ git+https://github.com/kashif/diffusers.git@main

# Windows only
tensorflow-cpu; platform_system == "Windows"
tensorflow-directml-plugin; platform_system == "Windows"
torch_scatter @ https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
torch_cluster @ https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"

# Linux only
deepspeed; platform_system == "Linux"
tensorflow; platform_system == "Linux"

# exllamav2
https://github.com/turboderp/exllamav2/releases/download/v0.0.16/exllamav2-0.0.16+cu121-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
https://github.com/turboderp/exllamav2/releases/download/v0.0.16/exllamav2-0.0.16+cu121-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"

# flash-attn (optional)
https://github.com/oobabooga/flash-attention/releases/download/v2.5.6/flash_attn-2.5.6+cu122torch2.2.0cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.11"
https://github.com/oobabooga/flash-attention/releases/download/v2.5.6/flash_attn-2.5.6+cu122torch2.2.0cxx11abiFALSE-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.6/flash_attn-2.5.6+cu122torch2.2cxx11abiFALSE-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.11"
https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.6/flash_attn-2.5.6+cu122torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.10"
